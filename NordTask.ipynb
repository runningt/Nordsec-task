{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e860673e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"NORD_Task\") \\\n",
    "      .getOrCreate() \n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "S3_BUCKET = 's3-nord-challenge-data'\n",
    "S3_REGION = 'eu-central-1'\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", f\"s3.{S3_REGION}.amazonaws.com\")\n",
    "\n",
    "\n",
    "\n",
    "#number of files to process - will be read as input\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75c20c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Note \n",
    "In general I can see 2 approaches to load files data:\n",
    "   - Approach 1.\n",
    "     - `spark.read.format('binaryFile').option(\"pathGlobFilter\",\"<path-glob>\").load(<s3-bucket>)`. This solution would read all files with metadata into single DataFrame (path, mod time,  length, content)\n",
    "     - parse content of file in apropriate resulted dataframe transformation\n",
    "   - The advantage of it is that you receive parallelized DataFrame, content of file would be read in lazy way during processing each file. So in theory on a big enough spark cluster spark should take care of distributing and performance for you. The problem seems to be when you have to work with pretty read milions of files with unknown file size. You may end up huge memory and performance issues. This problem is shown e.g. in [this blog article](https://wrightturn.wordpress.com/2015/07/22/getting-spark-data-from-aws-s3-using-boto-and-pyspark/). Although it's pretty old I did not find any more recent solution to the issue. It also describes second approach.\n",
    "   - Approach 2.\n",
    "       - list all objects you're interested files in s3 bucket into some collection (but without parallelizing it)\n",
    "       - create parallelized dataframe based on the given collection\n",
    "       - read and process file content as part of transformations\n",
    "   - The bottleneck might that you have to iterate over millions of files so the size of the collection to be processed (on one node) might be huge. \n",
    "   \n",
    "As I am not able to test on a large set of data and big enough spark cluster which approach is more efficient. I am going to use approach described in [mentioned article](https://wrightturn.wordpress.com/2015/07/22/getting-spark-data-from-aws-s3-using-boto-and-pyspark). However instead of using boto3 for listing all objects I will use [`hadoop.fs.path.getFilesystem.globStatus`](https://stackoverflow.com/a/67050173/2018369) because boto3 [seems to be not the most effective way](https://stackoverflow.com/q/69920805/2018369) to get file list.\n",
    "\n",
    "I was also considering one more approach, which however I could not find any good way to implement. So my idea was to create a dataframe similar to the one created by `spark.read.format('binaryFile').option(\"pathGlobFilter\",\"<path-glob>\").load(<s3-bucket>)`, but which contain only prefix of file (first 1024 or 2048 bytes). This way we could have a Dataframe(path, mod time,  length, PE headers), we could process the header of file to get all required PE metadata apart from imports/expors and in next step we could load apropriate sections of file to get imports/exports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c09b6fc9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clean_path = '/0/*.???'\n",
    "malware_path = '/1/*.???'\n",
    "\n",
    "\n",
    "\n",
    "cleanPath = sc._jvm.org.apache.hadoop.fs.Path(f's3a://{S3_BUCKET}{clean_path}')\n",
    "cFs = cleanPath.getFileSystem(hadoop_conf)\n",
    "clean_files = cFs.globStatus(cleanPath)\n",
    "\n",
    "malwarePath = sc._jvm.org.apache.hadoop.fs.Path(f's3a://{S3_BUCKET}{malware_path}')\n",
    "mFs = malwarePath.getFileSystem(hadoop_conf)\n",
    "malware_files = mFs.globStatus(malwarePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cdedf96f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14652\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JavaObject id=o46234"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "print(len(malware_files))\n",
    "files_to_process = random.sample(clean_files, int(N/2))+ random.sample(malware_files, int(N/2))\n",
    "print(len(files_to_process))\n",
    "\n",
    "malware_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea9cf6fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+\n",
      "|                path| size|type|\n",
      "+--------------------+-----+----+\n",
      "|/0/yBew1RCScL6IPp...|    0| dll|\n",
      "|/0/wFmMWSpwZSs018...| 1556| dll|\n",
      "|/0/NbO24Jqs5manwz...| 2560| dll|\n",
      "|/0/BQeyNvzPtZbUo4...| 4096| dll|\n",
      "|/1/YWBdWtgrl6BxYF...| 4096| exe|\n",
      "|/0/S6uvChiGpJzXwg...| 4096| dll|\n",
      "|/1/IMGRg0DkRBudYg...| 5120| dll|\n",
      "|/1/RwJEa3Fphb0MBE...| 6656| exe|\n",
      "|/0/jRUjmWjrcMTbRu...| 7680| DLL|\n",
      "|/0/DaUn6epfUwZBFa...| 9242| dll|\n",
      "|/0/LplBf5IViRpgCZ...| 9728| dll|\n",
      "|/0/tchpbDHf32pxYu...|11776| dll|\n",
      "|/0/w8GrAcrVBSApz7...|13824| dll|\n",
      "|/0/MMGPxOjrgbNgUb...|13840| dll|\n",
      "|/0/VRaYgpEEdysoq0...|22528| dll|\n",
      "|/0/v2Y0FOflJsSt42...|24576| dll|\n",
      "|/0/tOM2zfZl1crwBT...|24576| exe|\n",
      "|/0/MvxKp6jVjapuM7...|25600| exe|\n",
      "|/0/aC0BERV8SPcw5J...|26112| exe|\n",
      "|/1/4LeBU93eqe3JMy...|26112| exe|\n",
      "+--------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put files into dataFrame\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([       \n",
    "    StructField('path', StringType(), True),\n",
    "    StructField('size', IntegerType(), True),\n",
    "    StructField('type', StringType(), True)\n",
    "])\n",
    "data = [(f.getPath().toUri().getRawPath(), f.getLen(), f.getPath().getName().split('.')[-1]) for f in files_to_process]\n",
    "# data = [(f.getPath().toUri().getRawPath(), f.getLen(), f.getPath().getName().split('.')[-1]) for f in clean_files]\n",
    "# make sure we don't have duplicates\n",
    "filesDF= spark.createDataFrame(data=data, schema = schema).distinct()\n",
    "filesDF.orderBy(\"size\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c45018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+----+----+\n",
      "|path                                   |size|type|\n",
      "+---------------------------------------+----+----+\n",
      "|/0/yBew1RCScL6IPpDEjBBHoVhqTPDeLz3V.dll|0   |dll |\n",
      "|/0/kmpXko3MQvii5MesJhEFwn4j5PNjp0Nh.DLL|0   |DLL |\n",
      "|/0/fSp98VmWIff9hnv6fdlfo5uwYwczb7nd.exe|0   |exe |\n",
      "|/0/g6JNoxodcclCON5cDCH7nOLzY4tJEX2D.dll|115 |dll |\n",
      "|/0/N9qADLiJO4Z2GguKu2p0YRLENVFW1v1N.dll|115 |dll |\n",
      "|/0/oaraVTbrZMLISeMjnYcdbQlykwWOqqzT.dll|116 |dll |\n",
      "|/0/6cTLVriV86LLDgSLBJEViSvKwmBMQsh6.dll|116 |dll |\n",
      "|/0/yDKSqu4Jb9SiLF7cW1g8o0Cd14dG3GNJ.exe|127 |exe |\n",
      "|/0/T0sKa8Vg5a2vDyscxcHADXHYLTP3eFT2.dll|129 |dll |\n",
      "|/0/FMA3ScgaMpsXNz2NmgJY4vyiEhImw1QV.dll|130 |dll |\n",
      "|/0/hcrKj0lynV3fLptQPhcPb340fANniTkg.dll|131 |dll |\n",
      "|/0/yPixkkYO3Zm8PP8mx1V0n5IXcZJkrTdw.dll|131 |dll |\n",
      "|/0/emZYnrJcxMtYdBoEIR8EPE7ozfSheVZc.dll|132 |dll |\n",
      "|/0/bMXiMnwowm739HZP9bTs3xVu5g0KHGms.dll|132 |dll |\n",
      "|/0/5aer54Fijmdk8HDVm8rBplS1MhNb1Nyo.exe|132 |exe |\n",
      "|/0/ayZv4xeEzSVI9QFMaH6EzEOp5ewWygND.dll|133 |dll |\n",
      "|/0/KRbhZWynyDJ6wMOk31bJWBPbfShuiHE0.dll|133 |dll |\n",
      "|/0/LzZyF8lSrbJ75LbvJ8ueJwbB9vQ9r015.dll|134 |dll |\n",
      "|/0/tXvDK9UWF4baXoEs72emLVdgzIIrGxD8.exe|135 |exe |\n",
      "|/0/JSknOS0MHAEjB0bBF3enmGU13h88N6zI.dll|135 |dll |\n",
      "|/0/LTEuyBcRa0ODS5CJSg94rdtv7y40AVgK.dll|135 |dll |\n",
      "|/0/RpygY9HL26vPfBm7TGCM98GykHdvchuO.dll|136 |dll |\n",
      "|/0/FjoYD6LyjclE1wec8Btzs42dlPNnf0GP.exe|136 |exe |\n",
      "|/0/1keMYagcsjjlZFz9Cm0Swu40P0kd3jVw.dll|137 |dll |\n",
      "|/0/PoxFKJ62dqjm1fMOxAGrXYlUv64xWnKr.exe|137 |exe |\n",
      "|/0/AUX9Qfd2xhRXSTJdt4AJwO3n6CSlSMIf.exe|137 |exe |\n",
      "|/0/BmEVs2KMuDUQLJ9D3IuxlcNRHjIWrCKz.dll|138 |dll |\n",
      "|/0/XNL9fhFIzfAS7P1tHxv6JekUWnYHnUik.dll|138 |dll |\n",
      "|/0/MoW71dGQdSVWvzo2HAiMcN3YtUZ7jN20.dll|139 |dll |\n",
      "|/0/wUZmo3DdUjkPrIJfnvWLyDWc2xSncglR.dll|143 |dll |\n",
      "|/0/tkTNVDBDafMepp0OMxDyHMZBGcEajrHH.dll|143 |dll |\n",
      "|/0/bAz1S6NDEbL3UPq9wDF7PjcXoeRnhRZ5.dll|144 |dll |\n",
      "|/0/Xyi4s2fOEpRyScEaV35naSj7KfwYthf1.dll|147 |dll |\n",
      "|/0/K9zozKjdzAXRV2KCgvWMZ4Wxhz6i9tXM.dll|148 |dll |\n",
      "|/0/GHW3ZKvkloRD9Cc4g0ldVTLR5pXlBvvN.dll|148 |dll |\n",
      "|/0/bdxSLffWpb8U5xyWtoQEhulZFdFTDsFL.dll|148 |dll |\n",
      "|/0/o9x7id8LU1e6Ep71pc8eOKTsQXg1llve.dll|149 |dll |\n",
      "|/0/xLizeRZuQfFKsIuJtl7Z2qxUvwiEBNfp.dll|149 |dll |\n",
      "|/0/J5c71rGKeSyfDvQorqobwHolAqPBLMdE.dll|152 |dll |\n",
      "|/0/j3eVoE0gzFZ2963mmecHx8lawMrjDk5C.dll|158 |dll |\n",
      "|/0/LVaX6vvPOWDC1BpRGBnxo35Xr0hDOi0F.dll|159 |dll |\n",
      "|/0/tslHdocdSfkH5TQeTkfL5XYVCwk4dKoY.dll|163 |dll |\n",
      "|/0/ZyuRcq06mTHSuYRvrdYsDWCUqGp9aM0K.dll|173 |dll |\n",
      "|/0/6MI1Dcicq7UGRwshvUrXAwqKRpA8EX53.dll|174 |dll |\n",
      "|/0/eq5MSfUY9rPskYDpT5cM8O1kndXy5ixJ.exe|174 |exe |\n",
      "|/0/DwBP5r5Jn9XxsGHYpmFQd6wJdCYyrKL6.dll|174 |dll |\n",
      "|/0/j69rSiPSIGp9F2TnoRP8fB31byNO0CDl.exe|175 |exe |\n",
      "|/0/UYC3hr2DNklOu087rer9wxN3kgjatCNk.dll|175 |dll |\n",
      "|/0/pMclgmVOOr9Xn2TAgyr3zL2jOhKIiLJy.dll|175 |dll |\n",
      "|/0/baOG87k0rJyLLz8GHIvKUd485DQdav8n.dll|177 |dll |\n",
      "+---------------------------------------+----+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filesDF.orderBy('size').show(50,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef13288",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Database Notes\n",
    "I was considering SQL and NoSQL (key/value store) to store files info.\n",
    "\n",
    "### SQL Database - MySQL\n",
    "For now I decided to use simple solution  - MySQL database. This is pretty simple to replace with different SQL database or even with totally different solution in case of efficiency issues.\n",
    "\n",
    "The architecture is rather not complicated. All **distinct** file records are processed and stored in one table with following schema\n",
    " `path Varchar primary key, size Int, type Varchar, architecture Varchar default NULL, imports Int default NULL, exports Int default NULL, INDEX(size, type));`\n",
    "At the whole table is loaded into DataFrame. It is substracted from task files DF to ensure already processed files are skipped. And after processing transformed DF is appended to existing table in MySQL.\n",
    "\n",
    "Although number of files processed can reach (hundred of) millions [MySQL should handle it properly](https://dba.stackexchange.com/questions/20335/can-mysql-reasonably-perform-queries-on-billions-of-rows) with proper indexes. If there are billions of rows in DB we might start [encontering problems](https://stackoverflow.com/questions/38346613/mysql-and-a-table-with-100-millions-of-rows)\n",
    "In case of performance issue we may think about using different Database type.\n",
    "\n",
    "### NoSQL solutions\n",
    "I was considering also NoSQL database which very often perform better in distributed environment and in most cases scale horizontally much easier than classical SQL DB. For this task I consider key/value store as a good solution.\n",
    "\n",
    "#### Aerospike\n",
    "Aerospike was considered as it promises high efficiency, distributed (based on shared nothing architecture) database for storing key/value pairs. In commercial version it support pyspark distributed operations, direct import to RDDs etc. So if required it might give very good performance.\n",
    "\n",
    "#### Redis\n",
    "Open source, in-memory data store used as a database, cache, streaming engine, and message broker.\n",
    "\n",
    "\n",
    "### \"hybrid\" approach\n",
    "The issue with in-memory key/value store is that it does not provide (by default) persistence of data.\n",
    "This can be achieved both in Redis and Aerospike of course but not by default.\n",
    "\n",
    "My idea was to provide hybrid solution in which processed files data is stored in classical SQL database but apart from that it is also imported into key/value store. In that case there is no need to load all existing entries into DataFrame prior to processing new entries just to make sure some files weren't already processed. Instead, I would filter out from `filesDF` entries that exists in key/value store during transformations. As a last steps I would need to save (append) `filesDB` not only to SQL database but also to key/value store\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3825ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Settings\n",
    "jdbc_url = 'jdbc:mysql://db/nord_files'\n",
    "table = \"files_info\"\n",
    "username = \"root\"\n",
    "password = \"password\"\n",
    "driver = \"com.mysql.cj.jdbc.Driver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05ea7b13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['path', 'size', 'type']\n",
      "+----+----+----+\n",
      "|path|size|type|\n",
      "+----+----+----+\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get files already processed from database\n",
    "\n",
    "db_files_info = spark.read.format(\"jdbc\")\\\n",
    "    .options(url =jdbc_url,driver=driver, dbtable=table,user=username,password=password).load().select([\"path\", \"size\", \"type\"])\n",
    "\n",
    "\n",
    "print(db_files_info.columns)\n",
    "db_files_info.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b56b3135",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----+\n",
      "|                path|  size|type|\n",
      "+--------------------+------+----+\n",
      "|/1/G0VT13tsEjuIzj...|352256| exe|\n",
      "|/0/MMGPxOjrgbNgUb...| 13840| dll|\n",
      "|/0/TB8honzNnNXPKM...|509248| exe|\n",
      "|/0/ikES4VIEBUyNDM...| 26324| dll|\n",
      "|/1/tOysallBEHAUAM...| 94208| exe|\n",
      "|/0/Bb8cYk8FTQ59jz...|194048| dll|\n",
      "|/0/BIKElaZJq1tsUZ...|124928| dll|\n",
      "|/0/yBew1RCScL6IPp...|     0| dll|\n",
      "|/1/eksDCS3H2m7RY7...|572416| exe|\n",
      "|/1/ljzLUAWAxOBYjs...|259072| exe|\n",
      "|/0/BQeyNvzPtZbUo4...|  4096| dll|\n",
      "|/1/4LeBU93eqe3JMy...| 26112| exe|\n",
      "|/0/rgA1VwXwJxybiZ...| 63488| dll|\n",
      "|/1/5S1DzOMiFPhiD4...|450763| exe|\n",
      "|/1/mlBfSv1nduFzzG...|118784| exe|\n",
      "|/0/S6uvChiGpJzXwg...|  4096| dll|\n",
      "|/1/5jhmYTl9GoM8Ew...|224256| exe|\n",
      "|/1/j3MxwPUnTwKTzE...|384512| exe|\n",
      "|/0/HKB7N1d2XQS9UL...|103536| exe|\n",
      "|/0/0cwj8FURlEx0Zq...|202008| dll|\n",
      "+--------------------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove files that exists in DB from list of files to process\n",
    "filesDF = filesDF.subtract(db_files_info)\n",
    "filesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a02a2ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/usr/local/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.lock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.lock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/serializers.py:437\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:563\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#TODO: This should be fixed\u001b[39;00m\n\u001b[1;32m     16\u001b[0m parsed\u001b[38;5;241m=\u001b[39mfilesDF\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (\u001b[38;5;241m*\u001b[39mx, \u001b[38;5;241m*\u001b[39mPeParser(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m], reader\u001b[38;5;241m.\u001b[39mget_file_stream(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]),x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mget_short_meta()))\n\u001b[0;32m---> 18\u001b[0m parsedDF \u001b[38;5;241m=\u001b[39m \u001b[43mparsed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:66\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/sql/session.py:701\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[0;32m--> 701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    703\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/rdd.py:2620\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2614\u001b[0m \u001b[38;5;124;03m\"\"\" Return a JavaRDD of Object by unpickling\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \n\u001b[1;32m   2616\u001b[0m \u001b[38;5;124;03mIt will convert each Python object into Java object by Pyrolite, whenever the\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;124;03mRDD is serialized in batch or not.\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2619\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[0;32m-> 2620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/rdd.py:2951\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2949\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2951\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2953\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func,\n\u001b[1;32m   2954\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier)\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_val \u001b[38;5;241m=\u001b[39m python_rdd\u001b[38;5;241m.\u001b[39masJavaRDD()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/rdd.py:2830\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2829\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 2830\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[1;32m   2832\u001b[0m                               sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/rdd.py:2816\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   2815\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 2816\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   2818\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m         broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pyspark/serializers.py:447\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    446\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle '_thread.lock' object"
     ]
    }
   ],
   "source": [
    "# process files \n",
    "\n",
    "from s3file_reader import S3FileReader\n",
    "from pe_parser import PeParser\n",
    "#import importlib\n",
    "#importlib.reload(S3FileReader)\n",
    "\n",
    "reader = S3FileReader(S3_BUCKET, S3_REGION)\n",
    "\n",
    "schema = StructType(filesDF.schema.fields+[\n",
    "    StructField('architecture', StringType(), True),\n",
    "    StructField('imports', IntegerType(), True),\n",
    "    StructField('exports',IntegerType(), True)\n",
    "])\n",
    "#TODO: This should be fixed\n",
    "parsed=filesDF.rdd.map(lambda x: (*x, *PeParser(x['path'], reader.get_file_stream(x['path']),x['size']).get_short_meta()))\n",
    "\n",
    "parsedDF = parsed.toDF(schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29472c65",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Store result to DB\n",
    "parsedDF = filesDF\n",
    "parsedDF.write.format('jdbc').options(\n",
    "    url=jdbc_url, driver=driver,dbtable=table, user=username, password=password\n",
    ").mode('append').save()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
