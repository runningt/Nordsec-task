{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e860673e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"NORD_Task\") \\\n",
    "      .config(\"spark.redis.host\", \"cache\") \\\n",
    "      .config(\"spark.redis.port\", \"6379\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "#.config(\"spark.redis.auth\", \"password\") \\\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")\n",
    "\n",
    "S3_BUCKET = 's3-nord-challenge-data'\n",
    "S3_REGION = 'eu-central-1'\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", f\"s3.{S3_REGION}.amazonaws.com\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75c20c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Note \n",
    "In general I can see 2 approaches to load files data:\n",
    "   - Approach 1.\n",
    "     - `spark.read.format('binaryFile').option(\"pathGlobFilter\",\"<path-glob>\").load(<s3-bucket>)`. This solution would read all files with metadata into single DataFrame (path, mod time,  length, content)\n",
    "     - parse content of file in apropriate resulted dataframe transformation\n",
    "   - The advantage of it is that you receive parallelized DataFrame, content of file would be read in lazy way during processing each file. So in theory on a big enough spark cluster spark should take care of distributing and performance for you. The problem seems to be when you have to work with pretty read milions of files with unknown file size. You may end up huge memory and performance issues. This problem is shown e.g. in [this blog article](https://wrightturn.wordpress.com/2015/07/22/getting-spark-data-from-aws-s3-using-boto-and-pyspark/). Although it's pretty old I did not find any more recent solution to the issue. It also describes second approach.\n",
    "   - Approach 2.\n",
    "       - list all objects you're interested files in s3 bucket into some collection (but without parallelizing it)\n",
    "       - create parallelized dataframe based on the given collection\n",
    "       - read and process file content as part of transformations\n",
    "   - The bottleneck might that you have to iterate over millions of files so the size of the collection to be processed (on one node) might be huge. \n",
    "   \n",
    "As I am not able to test on a large set of data and big enough spark cluster which approach is more efficient. I am going to use approach described in [mentioned article](https://wrightturn.wordpress.com/2015/07/22/getting-spark-data-from-aws-s3-using-boto-and-pyspark). However instead of using boto3 for listing all objects I will use [`hadoop.fs.path.getFilesystem.globStatus`](https://stackoverflow.com/a/67050173/2018369) because boto3 [seems to be not the most effective way](https://stackoverflow.com/q/69920805/2018369) to get file list.\n",
    "\n",
    "I was also considering one more approach, which however I could not find any good way to implement. So my idea was to create a dataframe similar to the one created by `spark.read.format('binaryFile').option(\"pathGlobFilter\",\"<path-glob>\").load(<s3-bucket>)`, but which contain only prefix of file (first 1024 or 2048 bytes). This way we could have a Dataframe(path, mod time,  length, PE headers), we could process the header of file to get all required PE metadata apart from imports/expors and in next step we could load apropriate sections of file to get imports/exports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c09b6fc9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clean_path = '/0/*.???'\n",
    "malware_path = '/1/*.???'\n",
    "\n",
    "# number of files to process - will be read as input\n",
    "# N = 100\n",
    "N = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cleanPath = sc._jvm.org.apache.hadoop.fs.Path(f's3a://{S3_BUCKET}{clean_path}')\n",
    "cFs = cleanPath.getFileSystem(hadoop_conf)\n",
    "clean_files = cFs.globStatus(cleanPath)\n",
    "\n",
    "malwarePath = sc._jvm.org.apache.hadoop.fs.Path(f's3a://{S3_BUCKET}{malware_path}')\n",
    "mFs = malwarePath.getFileSystem(hadoop_conf)\n",
    "malware_files = mFs.globStatus(malwarePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdedf96f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14652\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JavaObject id=o645"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "print(len(malware_files))\n",
    "files_to_process = random.sample(clean_files, int(N/2))+ random.sample(malware_files, int(N/2))\n",
    "print(len(files_to_process))\n",
    "\n",
    "malware_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# put files into dataFrame\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "schema = StructType([       \n",
    "    StructField('path', StringType(), True),\n",
    "    StructField('size', IntegerType(), True),\n",
    "    StructField('type', StringType(), True)\n",
    "])\n",
    "data = [(f.getPath().toUri().getRawPath(), f.getLen(), f.getPath().getName().split('.')[-1]) for f in files_to_process]\n",
    "\n",
    "# make sure we don't have duplicates\n",
    "filesDF= spark.createDataFrame(data=data, schema = schema).distinct()\n",
    "filesDF.orderBy(\"size\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filesDF.orderBy('size').show(50,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Database Notes\n",
    "I was considering SQL and NoSQL (key/value store) to store files info. Finally **Hybrid approach was used**\n",
    "\n",
    "### SQL Database - MySQL\n",
    "The architecture is rather not complicated. All **distinct** file records are processed and stored in one table with following schema\n",
    " `path Varchar primary key, size Int, type Varchar, architecture Varchar default NULL, imports Int default NULL, exports Int default NULL, INDEX(size, type));`\n",
    "At the whole table is loaded into DataFrame. It is substracted from task files DF to ensure already processed files are skipped. And after processing transformed DF is appended to existing table in MySQL.\n",
    "\n",
    "Although number of files processed can reach (hundred of) millions [MySQL should handle it properly](https://dba.stackexchange.com/questions/20335/can-mysql-reasonably-perform-queries-on-billions-of-rows) with proper indexes. If there are billions of rows in DB we might start [encontering problems](https://stackoverflow.com/questions/38346613/mysql-and-a-table-with-100-millions-of-rows)\n",
    "In case of performance issue using different Database type might be considered as changing DB should be relatively easy. What should be changed in that case is `dataframe.write.` `format` and `options`\n",
    "\n",
    "### NoSQL solutions\n",
    "\n",
    "I was considering also NoSQL database which very often perform better in distributed environment and in most cases scale horizontally much easier than classical SQL DB. For this task I consider key/value store as a good solution.\n",
    "\n",
    "#### Aerospike\n",
    "Aerospike was considered as it promises high efficiency, distributed (based on shared nothing architecture) database for storing key/value pairs. In commercial version it support pyspark distributed operations, direct import to RDDs etc. So if required it might give very good performance.\n",
    "\n",
    "#### Redis\n",
    "Open source, in-memory data store used as a database, cache, streaming engine, and message broker.\n",
    "\n",
    "\n",
    "### \"hybrid\" approach - caching\n",
    "The issue with in-memory key/value store is that it does not provide (by default) persistence of data.\n",
    "This can be achieved both in Redis and Aerospike of course but not by default.\n",
    "\n",
    "My idea is to provide hybrid solution in which processed files data is stored in classical SQL database but apart from that it is also imported into key/value store. In that case there is no need to load all existing entries into DataFrame prior to processing new entries just to make sure some files weren't already processed. Instead,  `filesDF` entries that exists in key/value store should be filtered during transformation. As a last steps  `filesDB`should be saved (appended) not only to SQL database but also to key/value store\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get files already processed from redis cache\n",
    "\n",
    "redis_files_info = spark.read.format(\"org.apache.spark.sql.redis\").schema(schema)\\\n",
    "    .option(\"table\", \"s3\").option(\"key.column\", \"path\").load()\n",
    "\n",
    "\n",
    "redis_files_info.show()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove files that exists in DB from list of files to process\n",
    "filesDF = filesDF.subtract(redis_files_info)\n",
    "filesDF.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# process files \n",
    "from functools import partial\n",
    "from spark_utils import parse_file\n",
    "\n",
    "schema_with_meta = StructType(filesDF.schema.fields+[\n",
    "    StructField('architecture', StringType(), True),\n",
    "    StructField('imports', IntegerType(), True),\n",
    "    StructField('exports',IntegerType(), True)\n",
    "])\n",
    "\n",
    "parsed=filesDF.rdd.map(partial(parse_file, bucket=S3_BUCKET, region=S3_REGION))\n",
    "\n",
    "parsedDF = parsed.toDF(schema_with_meta)\n",
    "parsedDF.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37203e76",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cef359cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<botocore.response.StreamingBody object at 0x7f676b6c2290>          (0 + 1) / 1]\n",
      "<botocore.response.StreamingBody object at 0x7f676ab8a7a0>\n",
      "<botocore.response.StreamingBody object at 0x7f676c9ea5f0>\n",
      "<botocore.response.StreamingBody object at 0x7f676ca04a00>\n",
      "<botocore.response.StreamingBody object at 0x7f676d2db400>\n",
      "<botocore.response.StreamingBody object at 0x7f676ab73a00>\n",
      "<botocore.response.StreamingBody object at 0x7f676ad336d0>\n",
      "<botocore.response.StreamingBody object at 0x7f676a405db0>\n",
      "<botocore.response.StreamingBody object at 0x7f676a449870>\n",
      "<botocore.response.StreamingBody object at 0x7f6769ee76d0>\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store result to DB\n",
    "parsedDF.write.format('jdbc').options(\n",
    "    url=jdbc_url, driver=driver,dbtable=table, user=username, password=password\n",
    ").mode('append').save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce07387e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<botocore.response.StreamingBody object at 0x7f676aae3550>          (0 + 1) / 1]\n",
      "<botocore.response.StreamingBody object at 0x7f676aa99e70>\n",
      "<botocore.response.StreamingBody object at 0x7f676eefa2f0>\n",
      "<botocore.response.StreamingBody object at 0x7f676ef306d0>\n",
      "<botocore.response.StreamingBody object at 0x7f676ef770d0>\n",
      "<botocore.response.StreamingBody object at 0x7f676a9efe80>\n",
      "<botocore.response.StreamingBody object at 0x7f676a7a0c70>\n",
      "<botocore.response.StreamingBody object at 0x7f676a715ab0>\n",
      "<botocore.response.StreamingBody object at 0x7f676a759570>\n",
      "<botocore.response.StreamingBody object at 0x7f6769dff3a0>\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Store results also to Redis cache\n",
    "parsedDF.select([\"path\",\"size\", \"type\"]).write.format(\"org.apache.spark.sql.redis\").option(\"table\",\"s3\").option(\"key.column\", \"path\").mode('append').save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}